
		<style>
            * {
                /* font-family: "Open Sans"; */
                font-family: "Source Sans Pro", Helvetica, sans-serif;
                box-sizing :  content-box;
            }
            body {
                background-color: transparent;
            }
        </style>
		
<!doctype html>
<html lang="en">
<head>
  <script>
	window.history.replaceState = function(){ };
  </script>

  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/4.4.0/reveal.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/4.4.0/theme/serif.min.css" id="theme">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/4.4.0/theme/white.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/4.4.0/plugin/highlight/zenburn.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/4.4.0/plugin/highlight/monokai.min.css">
  
<!--
  <link rel="stylesheet" href="https://layout.xbdev.net/var/images/pdf.css">
-->
  <style>
  b, strong {
     color: red;
  }
  h1,h2,h3,h4,h5 {
     color:unset!important;
  }
  .hljs {
     font-size:105%;
  }
  </style>

</head>
<body>
    <div class="reveal">
      <div style='position:absolute;left:10px;top:10px;font-size:8pt;color:transparent;'>https://layout.xbdev.net</div>
      <div class="slides" style='border:0px solid green;'>
		<!--
        <section data-background-color="rgb(70, 70, 255)"><h2>Welcome</h2></section>
		<section style="height:720px; background-color:rgb(70, 70, 255);"><h2>How are you?</h2></section>
		-->
        
<section data-markdown 
         data-transition="fade" 
         data-transition-speed="slow"
         data-background-video="" 
         data-background-video-loop data-background-video-muted 
         data-background-opacity=""
         data-background-image=''
         data-background="linear-gradient(90deg, black 0%, rgba(0,0,0,0.2) 80%, transparent 100%),url('https://testingtraveler.com/wp-content/uploads/2021/03/review.jpg')  50% 50%/contain no-repeat" 
		 data-background-color="white"
         data-background-size="cover"
         class=""
		 style="color:white"
>
<textarea data-template>
     
	


<!--{border:0px solid green}-->

<!--{color:white}-->
<!--{background:https://testingtraveler.com/wp-content/uploads/2021/03/review.jpg, 50% 50%/contain no-repeat}-->
<!--{gradient:90deg, black 0%, rgba(0,0,0,0.2) 80%, transparent 100%}-->


# Evaluation Metrics
**Data Mining and Machine Learning**




</textarea>
</section>
        
<section data-markdown 
         data-transition="fade" 
         data-transition-speed="slow"
         data-background-video="" 
         data-background-video-loop data-background-video-muted 
         data-background-opacity=""
         data-background-image=''
         data-background="" 
		 data-background-color="#f9dc24"
         data-background-size="cover"
         class=""
		 style=""
>
<textarea data-template>
     
	

<div style='font-size:90%'>

# Overview
 - What are evaluation metrics?
 - Common evaluation metrics:
   - Accuracy, precision, recall, and F1 score for classification models
 - Receiver Operating Characteristic (ROC) curves and Area Under the Curve (AUC)
 - Confusion matrix and classification metrics (e.g., True Positive, False Negative)
 - Evaluation measures for regression models (e.g., Mean Squared Error, R-squared)
 - Summary/discussion

</div>

</textarea>
</section>
        
<section data-markdown 
         data-transition="fade" 
         data-transition-speed="slow"
         data-background-video="" 
         data-background-video-loop data-background-video-muted 
         data-background-opacity=""
         data-background-image=''
         data-background="" 
		 data-background-color="#3553bc"
         data-background-size="cover"
         class=""
		 style=""
>
<textarea data-template>
     
	

# What are evaluation metrics?

---

## What are evaluation metrics?

Evaluation metrics are measures used to assess the performance and effectiveness of a machine learning model. They provide quantitative insights into how well the model is performing on a given task.

</textarea>
</section>
        
<section data-markdown 
         data-transition="fade" 
         data-transition-speed="slow"
         data-background-video="" 
         data-background-video-loop data-background-video-muted 
         data-background-opacity=""
         data-background-image=''
         data-background="" 
		 data-background-color="#f9dc24"
         data-background-size="cover"
         class=""
		 style=""
>
<textarea data-template>
     
	

# What is accuracy?

---

## What is accuracy?

Accuracy is a commonly used evaluation metric that measures the proportion of correct predictions made by a model over the total number of predictions. It is calculated as the ratio of the number of correct predictions to the total number of predictions.

</textarea>
</section>
        
<section data-markdown 
         data-transition="fade" 
         data-transition-speed="slow"
         data-background-video="" 
         data-background-video-loop data-background-video-muted 
         data-background-opacity=""
         data-background-image=''
         data-background="" 
		 data-background-color="#3553bc"
         data-background-size="cover"
         class=""
		 style=""
>
<textarea data-template>
     
	

# What are precision and recall?

---

## What are precision and recall?

Precision and recall are evaluation metrics used for binary classification tasks.
    - Precision measures the proportion of true positive predictions out of all positive predictions.
    - Recall measures the proportion of true positive predictions out of all actual positive instances in the data.

</textarea>
</section>
        
<section data-markdown 
         data-transition="fade" 
         data-transition-speed="slow"
         data-background-video="" 
         data-background-video-loop data-background-video-muted 
         data-background-opacity=""
         data-background-image=''
         data-background="" 
		 data-background-color="#f9dc24"
         data-background-size="cover"
         class=""
		 style=""
>
<textarea data-template>
     
	

# What is the F1 score?

---

## What is the F1 score?

The F1 score is a metric that combines precision and recall into a single value. It is the harmonic mean of precision and recall, providing a balanced measure of a model's performance on both positive and negative instances.

</textarea>
</section>
        
<section data-markdown 
         data-transition="fade" 
         data-transition-speed="slow"
         data-background-video="" 
         data-background-video-loop data-background-video-muted 
         data-background-opacity=""
         data-background-image=''
         data-background="" 
		 data-background-color="#3553bc"
         data-background-size="cover"
         class=""
		 style=""
>
<textarea data-template>
     
	

# What is a confusion matrix?

---

## What is a confusion matrix?

A confusion matrix is a table that summarizes the performance of a classification model. It provides a detailed breakdown of the number of true positive, true negative, false positive, and false negative predictions made by the model.

</textarea>
</section>
        
<section data-markdown 
         data-transition="fade" 
         data-transition-speed="slow"
         data-background-video="" 
         data-background-video-loop data-background-video-muted 
         data-background-opacity=""
         data-background-image=''
         data-background="" 
		 data-background-color="#f9dc24"
         data-background-size="cover"
         class=""
		 style=""
>
<textarea data-template>
     
	

# What is the ROC curve? 

---

## What is the ROC curve? 

The **Receiver Operating Characteristic (ROC) Curve** is a graphical representation of a binary classification model's performance at various classification thresholds. It plots the true positive rate (TPR) against the false positive rate (FPR) as the threshold for classifying positive instances is varied.

</textarea>
</section>
        
<section data-markdown 
         data-transition="fade" 
         data-transition-speed="slow"
         data-background-video="" 
         data-background-video-loop data-background-video-muted 
         data-background-opacity=""
         data-background-image=''
         data-background="" 
		 data-background-color="#3553bc"
         data-background-size="cover"
         class=""
		 style=""
>
<textarea data-template>
     
	

# What is the AUC?

---

## What is the AUC?

The **Area Under the Curve (AUC)** is a metric that quantifies the overall performance of a binary classification model based on its ROC curve. It represents the probability that a randomly chosen positive instance will be ranked higher than a randomly chosen negative instance.

</textarea>
</section>
        
<section data-markdown 
         data-transition="fade" 
         data-transition-speed="slow"
         data-background-video="" 
         data-background-video-loop data-background-video-muted 
         data-background-opacity=""
         data-background-image=''
         data-background="" 
		 data-background-color="#f9dc24"
         data-background-size="cover"
         class=""
		 style=""
>
<textarea data-template>
     
	

# What is the Mean Absolute Error (MAE)? 

---

## What is the Mean Absolute Error (MAE)? 

The Mean Absolute Error (MAE) is an evaluation metric used for regression tasks. It measures the average absolute difference between the predicted and actual values. It provides a measure of how far, on average, the predictions deviate from the true values.

</textarea>
</section>
        
<section data-markdown 
         data-transition="fade" 
         data-transition-speed="slow"
         data-background-video="" 
         data-background-video-loop data-background-video-muted 
         data-background-opacity=""
         data-background-image=''
         data-background="" 
		 data-background-color="#3553bc"
         data-background-size="cover"
         class=""
		 style=""
>
<textarea data-template>
     
	

# What is the Mean Squared Error (MSE)?

---

## What is the Mean Squared Error (MSE)?

The Mean Squared Error (MSE) is another evaluation metric used for regression tasks. It measures the average squared difference between the predicted and actual values. It gives higher weight to larger errors compared to MAE.

</textarea>
</section>
        
<section data-markdown 
         data-transition="fade" 
         data-transition-speed="slow"
         data-background-video="" 
         data-background-video-loop data-background-video-muted 
         data-background-opacity=""
         data-background-image=''
         data-background="" 
		 data-background-color="#f9dc24"
         data-background-size="cover"
         class=""
		 style=""
>
<textarea data-template>
     
	

# What is the R-squared (R^2) score?

---

## What is the R-squared (R^2) score?

The R-squared (R^2) score is a metric used for regression tasks that measures the proportion of the variance in the dependent variable that can be explained by the independent variables. It ranges from 0 to 1, with 1 indicating a perfect fit.

</textarea>
</section>
        
<section data-markdown 
         data-transition="fade" 
         data-transition-speed="slow"
         data-background-video="" 
         data-background-video-loop data-background-video-muted 
         data-background-opacity=""
         data-background-image=''
         data-background="" 
		 data-background-color="#3553bc"
         data-background-size="cover"
         class=""
		 style=""
>
<textarea data-template>
     
	

# What is the Precision-Recall (PR) curve?

---

## What is the Precision-Recall (PR) curve?

The Precision-Recall curve is a graphical representation of the trade-off between precision and recall for different classification thresholds. It plots the precision against the recall as the threshold for classifying positive instances is varied.

</textarea>
</section>
        
<section data-markdown 
         data-transition="fade" 
         data-transition-speed="slow"
         data-background-video="" 
         data-background-video-loop data-background-video-muted 
         data-background-opacity=""
         data-background-image=''
         data-background="" 
		 data-background-color="#f9dc24"
         data-background-size="cover"
         class=""
		 style=""
>
<textarea data-template>
     
	

# What is the Average Precision (AP)?

---

## What is the Average Precision (AP)?

The Average Precision (AP) is a metric used to evaluate the performance of a model based on the Precision-Recall curve. It calculates the average precision across all recall levels, providing a single scalar value to quantify the model's performance.

</textarea>
</section>
        
<section data-markdown 
         data-transition="fade" 
         data-transition-speed="slow"
         data-background-video="" 
         data-background-video-loop data-background-video-muted 
         data-background-opacity=""
         data-background-image=''
         data-background="" 
		 data-background-color="#3553bc"
         data-background-size="cover"
         class=""
		 style=""
>
<textarea data-template>
     
	

# What is Cohen's Kappa?

---

## What is Cohen's Kappa?

Cohen's Kappa is a metric used to assess the agreement between two annotators or models. It measures the agreement beyond what would be expected by chance. It is commonly used when evaluating models for tasks such as sentiment analysis or object detection.

</textarea>
</section>
        
<section data-markdown 
         data-transition="fade" 
         data-transition-speed="slow"
         data-background-video="" 
         data-background-video-loop data-background-video-muted 
         data-background-opacity=""
         data-background-image=''
         data-background="" 
		 data-background-color="#f9dc24"
         data-background-size="cover"
         class=""
		 style=""
>
<textarea data-template>
     
	

# What is cross-validation?

---

## What is cross-validation?

Cross-validation is a technique used to assess the performance of a model on unseen data. It involves splitting the data into multiple subsets, training the model on one subset, and evaluating it on the remaining subsets. It helps estimate the model's generalization ability and mitigate overfitting.

</textarea>
</section>
        
<section data-markdown 
         data-transition="fade" 
         data-transition-speed="slow"
         data-background-video="" 
         data-background-video-loop data-background-video-muted 
         data-background-opacity=""
         data-background-image=''
         data-background="" 
		 data-background-color="#3553bc"
         data-background-size="cover"
         class=""
		 style=""
>
<textarea data-template>
     
	

# Summary

- Review **key evaluation metrics**
- Emphasis on the importance of selecting appropriate metrics based on the specific task and requirements.
- Encouragement to use a combination of evaluation metrics to gain a comprehensive understanding of the model's performance.









</textarea>
</section>
        
      </div>
    </div>
    

    <script src="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/4.4.0/reveal.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/4.4.0/plugin/markdown/markdown.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/4.4.0/plugin/highlight/highlight.min.js"></script>



	<script src="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/4.4.0/plugin/math/math.min.js"></script>
<!--
	<script src="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/4.4.0/plugin/math/katex.min.js"></script>
-->

    <script>
    window.addEventListener('load', function() {
      console.log('setting up reveal');
	  //window.history.pushState(null, null, '?print-pdf');

      Reveal.initialize( { width: 1240,
  						   height: 720,
						   controls: true,
						   progress: true,
						   center: true,
						   hash: true,
                           // transition: 'fade', // none/fade/slide/convex/concave/zoom
						   controlsTutorial: true,
						   transitionSpeed: 'slow',
                           plugins: [ RevealMarkdown, RevealHighlight, RevealMath.KaTeX, RevealChalkboard  ]
                         } );
    });
    </script>
    
    <link rel="stylesheet" href="https://courses.xbdev.net/var/styles/revealjschalkboard.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.8.2/css/all.min.css">    
    <script src="https://courses.xbdev.net/var/scripts/revealjschalkboard.js"></script>
    <!--
    RevealChalkboard
    -->

</body>
</html>
	