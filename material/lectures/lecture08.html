
		<style>
            * {
                /* font-family: "Open Sans"; */
                font-family: "Source Sans Pro", Helvetica, sans-serif;
                box-sizing :  content-box;
            }
            body {
                background-color: transparent;
            }
        </style>
		
<!doctype html>
<html lang="en">
<head>
  <script>
	window.history.replaceState = function(){ };
  </script>

  <script src='https://f21dl.github.io/material/scripts/common.js'></script>
  
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/4.4.0/reveal.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/4.4.0/theme/serif.min.css" id="theme">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/4.4.0/theme/white.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/4.4.0/plugin/highlight/zenburn.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/4.4.0/plugin/highlight/monokai.min.css">
  
<!--
  <link rel="stylesheet" href="https://layout.xbdev.net/var/images/pdf.css">
-->
  <style>
  b, strong {
     color: red;
  }
  h1,h2,h3,h4,h5 {
     color:unset!important;
  }
  .hljs {
     font-size:105%;
  }
  </style>

</head>
<body>
    <div class="reveal">
      <div style='position:absolute;left:10px;top:10px;font-size:8pt;color:transparent;'>https://layout.xbdev.net</div>
      <div class="slides" style='border: 0px solid orange;'>
		<!--
        <section data-background-color="rgb(70, 70, 255)"><h2>Welcome</h2></section>
		<section style="height:720px; background-color:rgb(70, 70, 255);"><h2>How are you?</h2></section>
		-->
        
<section data-markdown 
         data-transition="fade" 
         data-transition-speed="slow"
         data-background-video="" 
         data-background-video-loop data-background-video-muted 
         data-background-opacity=""
         data-background-image=''
         data-background="linear-gradient(90deg,black, black 10%, rgba(0,0,0,0.2) 100% ),url('https://blog.equinix.com/wp-content/uploads/2019/03/robot-trees-green-sustainability-ai.jpg') 100% 100%/cover" 
		 data-background-color="#3553bc"
         data-background-size="cover"
         class=""
		 style="color:white"
>
<textarea data-template>
     
	

<!--{border: 0px solid orange}-->
<!--{background:https://blog.equinix.com/wp-content/uploads/2019/03/robot-trees-green-sustainability-ai.jpg}-->
<!--{gradient:90deg,black, black 10%, rgba(0,0,0,0.2) 100% }-->
					

# Decision Trees
## Data Mining & Machine Learning


</textarea>
</section>
        
<section data-markdown 
         data-transition="fade" 
         data-transition-speed="slow"
         data-background-video="" 
         data-background-video-loop data-background-video-muted 
         data-background-opacity=""
         data-background-image=''
         data-background="" 
		 data-background-color="#f9dc24"
         data-background-size="cover"
         class=""
		 style=""
>
<textarea data-template>
     
	


<div style='font-size:70%'>

# Reading Material

<!-- -------------------------------------- -->
<div style='background-color:rgb(100,155,255); padding:10px; height:160px; '>
<span style='display:inline-block;width:900px;vertical-align:top;'>

[Hands-On Machine Learning with Scikit-Learn and TensorFlow](https://github.com/ageron/handson-ml2)
* Ch6: Decision Trees

</span>
<img style='margin:0px; max-height:100%;' src='https://camo.githubusercontent.com/27a6f1d2f26314cdbd8d84090aeb8b5d3a56230bdaafc72ec879588728fa4f9a/68747470733a2f2f696d616765732d6e612e73736c2d696d616765732d616d617a6f6e2e636f6d2f696d616765732f492f353161715963315179724c2e5f53583337395f424f312c3230342c3230332c3230305f2e6a7067'>

</div>
<!-- ------------------------------------- -->
<div style='background-color:rgb(100,155,155); padding:10px; height:200px; '>

<img style='margin:0px; height:100%;max-height:100%;' src='https://m.media-amazon.com/images/I/51xkbNga5kL._SX403_BO1,204,203,200_.jpg'>

<span style='display:inline-block;width:900px;vertical-align:top;'>

[Data Mining:Practical Machine Learning Tools and Techniques](https://www.cs.waikato.ac.nz/ml/weka/book.html)
* Ch3: Output: Knowledge representation
* Ch4: Algorithms: the basic methods
* Ch6: Trees and rules

</span>
</div>
<!-- ------------------------------------- -->
<div style='background-color:rgb(100,55,155); padding:10px; height:160px; '>
<span style='display:inline-block;width:900px;vertical-align:top;'>

[Data Mining and Machine Learning: Fundamental Concepts and Algorithms](https://dataminingbook.info/)
* Part I: Data Analysis Foundations

</span>
<img style='margin:0px; height:100%;max-height:100%;' src='https://m.media-amazon.com/images/I/51Nk67LGBQL.jpg'>

</div>
<!-- -------------------------------------- -->
</div>


</textarea>
</section>
        
<section data-markdown 
         data-transition="fade" 
         data-transition-speed="slow"
         data-background-video="" 
         data-background-video-loop data-background-video-muted 
         data-background-opacity=""
         data-background-image=''
         data-background="" 
		 data-background-color="#3553bc"
         data-background-size="cover"
         class=""
		 style=""
>
<textarea data-template>
     
	

# Overview
Decision Trees are powerful and widely used algorithms in data mining and machine learning that can handle both classification and regression tasks. They are non-parametric models that recursively partition the data into subsets based on the values of input features. Decision Trees offer intuitive interpretations, are easy to visualize, and can handle both categorical and numerical data.

</textarea>
</section>
        
<section data-markdown 
         data-transition="fade" 
         data-transition-speed="slow"
         data-background-video="" 
         data-background-video-loop data-background-video-muted 
         data-background-opacity=""
         data-background-image=''
         data-background="" 
		 data-background-color="#f9dc24"
         data-background-size="cover"
         class=""
		 style=""
>
<textarea data-template>
     
	

Summary of Decision Trees:
* Tree Structure
* Decision Rule
* Feature Selection
* Pruning
* Handling Missing Data
* Ensemble Methods
* Interpretability
* Limitations
* Applications


</textarea>
</section>
        
<section data-markdown 
         data-transition="fade" 
         data-transition-speed="slow"
         data-background-video="" 
         data-background-video-loop data-background-video-muted 
         data-background-opacity=""
         data-background-image=''
         data-background="" 
		 data-background-color="#3553bc"
         data-background-size="cover"
         class=""
		 style=""
>
<textarea data-template>
     
	


## Tree Structure
A Decision Tree is a hierarchical structure comprising nodes and edges. The top node is called the root, which represents the entire dataset. Each internal node corresponds to a feature, and each edge represents a decision or split based on the feature's value. The leaf nodes represent the final predictions or outcomes.

</textarea>
</section>
        
<section data-markdown 
         data-transition="fade" 
         data-transition-speed="slow"
         data-background-video="" 
         data-background-video-loop data-background-video-muted 
         data-background-opacity=""
         data-background-image=''
         data-background="" 
		 data-background-color="#f9dc24"
         data-background-size="cover"
         class=""
		 style=""
>
<textarea data-template>
     
	

## Decision Rule
At each node, a decision rule is applied to determine the best feature and value to split the data. The goal is to maximize the purity of each subset, making the classes or values within each subset as homogeneous as possible.

</textarea>
</section>
        
<section data-markdown 
         data-transition="fade" 
         data-transition-speed="slow"
         data-background-video="" 
         data-background-video-loop data-background-video-muted 
         data-background-opacity=""
         data-background-image=''
         data-background="" 
		 data-background-color="#3553bc"
         data-background-size="cover"
         class=""
		 style=""
>
<textarea data-template>
     
	

## Feature Selection
Decision Trees use various criteria to select the best feature for splitting, such as Gini impurity, entropy, or information gain for classification problems, and mean squared error (MSE) or mean absolute error (MAE) for regression problems.

</textarea>
</section>
        
<section data-markdown 
         data-transition="fade" 
         data-transition-speed="slow"
         data-background-video="" 
         data-background-video-loop data-background-video-muted 
         data-background-opacity=""
         data-background-image=''
         data-background="" 
		 data-background-color="#f9dc24"
         data-background-size="cover"
         class=""
		 style=""
>
<textarea data-template>
     
	

## Pruning
To avoid overfitting, Decision Trees can be pruned by removing nodes that do not contribute significantly to the model's predictive power. Pruning helps prevent the tree from becoming too complex and better generalizes to new data.

</textarea>
</section>
        
<section data-markdown 
         data-transition="fade" 
         data-transition-speed="slow"
         data-background-video="" 
         data-background-video-loop data-background-video-muted 
         data-background-opacity=""
         data-background-image=''
         data-background="" 
		 data-background-color="#3553bc"
         data-background-size="cover"
         class=""
		 style=""
>
<textarea data-template>
     
	

## Handling Missing Data
Decision Trees can handle missing data effectively by selecting the best available feature and value for the split, even if some values are missing for a particular instance.

</textarea>
</section>
        
<section data-markdown 
         data-transition="fade" 
         data-transition-speed="slow"
         data-background-video="" 
         data-background-video-loop data-background-video-muted 
         data-background-opacity=""
         data-background-image=''
         data-background="" 
		 data-background-color="#f9dc24"
         data-background-size="cover"
         class=""
		 style=""
>
<textarea data-template>
     
	

## Ensemble Methods
Ensemble methods like Random Forests and Gradient Boosting utilize multiple Decision Trees to improve predictive accuracy and reduce overfitting. Random Forests build several trees and make predictions by averaging the results, while Gradient Boosting builds trees sequentially to correct the errors of the previous ones.

</textarea>
</section>
        
<section data-markdown 
         data-transition="fade" 
         data-transition-speed="slow"
         data-background-video="" 
         data-background-video-loop data-background-video-muted 
         data-background-opacity=""
         data-background-image=''
         data-background="" 
		 data-background-color="#3553bc"
         data-background-size="cover"
         class=""
		 style=""
>
<textarea data-template>
     
	

## Interpretability
Decision Trees offer transparency and interpretability, making them useful for understanding how the model arrived at its predictions. The decision paths can be visualized, and feature importance can be extracted.

</textarea>
</section>
        
<section data-markdown 
         data-transition="fade" 
         data-transition-speed="slow"
         data-background-video="" 
         data-background-video-loop data-background-video-muted 
         data-background-opacity=""
         data-background-image=''
         data-background="" 
		 data-background-color="#f9dc24"
         data-background-size="cover"
         class=""
		 style=""
>
<textarea data-template>
     
	

## Limitations
Decision Trees can suffer from overfitting, especially if the tree is too deep and complex. They may not capture complex relationships in the data as effectively as other algorithms like neural networks.

</textarea>
</section>
        
<section data-markdown 
         data-transition="fade" 
         data-transition-speed="slow"
         data-background-video="" 
         data-background-video-loop data-background-video-muted 
         data-background-opacity=""
         data-background-image=''
         data-background="" 
		 data-background-color="#3553bc"
         data-background-size="cover"
         class=""
		 style=""
>
<textarea data-template>
     
	

## Applications
Decision Trees are applied in various domains, including healthcare (diagnosis), finance (credit risk assessment), marketing (customer segmentation), and natural language processing (sentiment analysis), among others.



</textarea>
</section>
        
<section data-markdown 
         data-transition="fade" 
         data-transition-speed="slow"
         data-background-video="" 
         data-background-video-loop data-background-video-muted 
         data-background-opacity=""
         data-background-image=''
         data-background="" 
		 data-background-color="#f9dc24"
         data-background-size="cover"
         class=""
		 style=""
>
<textarea data-template>
     
	

# Summary

Decision Trees are versatile and can be used for both simple and complex tasks. They are especially beneficial for exploratory data analysis, initial model prototyping, and tasks where interpretability is crucial. When combined with ensemble methods, Decision Trees can become even more robust and accurate.

































</textarea>
</section>
        
      </div>
    </div>
    

    <script src="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/4.4.0/reveal.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/4.4.0/plugin/markdown/markdown.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/4.4.0/plugin/highlight/highlight.min.js"></script>



	<script src="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/4.4.0/plugin/math/math.min.js"></script>
<!--
	<script src="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/4.4.0/plugin/math/katex.min.js"></script>
-->

    <script>
    window.addEventListener('load', function() {
      console.log('setting up reveal');
	  //window.history.pushState(null, null, '?print-pdf');

      Reveal.initialize( { width: 1240,
  						   height: 720,
						   controls: true,
						   progress: true,
						   center: true,
						   hash: true,
                           // transition: 'fade', // none/fade/slide/convex/concave/zoom
						   controlsTutorial: true,
						   transitionSpeed: 'slow',
                           plugins: [ RevealMarkdown, RevealHighlight, RevealMath.KaTeX, RevealChalkboard  ]
                         } );
    });
    </script>
    
    <link rel="stylesheet" href="https://courses.xbdev.net/var/styles/revealjschalkboard.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.8.2/css/all.min.css">    
    <script src="https://courses.xbdev.net/var/scripts/revealjschalkboard.js"></script>
    <!--
    RevealChalkboard
    -->

</body>
</html>
	